{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d5742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd55d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "model = Sequential([Dense(64, activation='relu'), Dense(10)])\n",
    "\n",
    "# Custom training loop\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Load or create your training dataset here\n",
    "# Example:\n",
    "x_train = tf.random.uniform((100, 10)) # Replace with your actual training data\n",
    "y_train = tf.random.uniform((100,), maxval=10, dtype=tf.int64) # Replace with your actual training labels\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47151021",
   "metadata": {},
   "source": [
    "# Custom Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc84f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.197458505630493\n",
      "Epoch: 2, Loss: 2.1741411685943604\n",
      "Epoch: 3, Loss: 2.1487271785736084\n",
      "Epoch: 4, Loss: 2.1239023208618164\n",
      "Epoch: 5, Loss: 2.099289894104004\n",
      "Epoch: 6, Loss: 2.0751993656158447\n",
      "Epoch: 7, Loss: 2.051640510559082\n",
      "Epoch: 8, Loss: 2.0284149646759033\n",
      "Epoch: 9, Loss: 2.005481004714966\n",
      "Epoch: 10, Loss: 1.9828810691833496\n",
      "Epoch: 11, Loss: 1.960989236831665\n",
      "Epoch: 12, Loss: 1.9398070573806763\n",
      "Epoch: 13, Loss: 1.919180154800415\n",
      "Epoch: 14, Loss: 1.8992009162902832\n",
      "Epoch: 15, Loss: 1.8795537948608398\n",
      "Epoch: 16, Loss: 1.8603683710098267\n",
      "Epoch: 17, Loss: 1.841550588607788\n",
      "Epoch: 18, Loss: 1.82290780544281\n",
      "Epoch: 19, Loss: 1.804571509361267\n",
      "Epoch: 20, Loss: 1.7862699031829834\n",
      "Epoch: 21, Loss: 1.7680091857910156\n",
      "Epoch: 22, Loss: 1.7496975660324097\n",
      "Epoch: 23, Loss: 1.7313973903656006\n",
      "Epoch: 24, Loss: 1.71377694606781\n",
      "Epoch: 25, Loss: 1.696017861366272\n",
      "Epoch: 26, Loss: 1.6784679889678955\n",
      "Epoch: 27, Loss: 1.6604732275009155\n",
      "Epoch: 28, Loss: 1.6425724029541016\n",
      "Epoch: 29, Loss: 1.6246154308319092\n",
      "Epoch: 30, Loss: 1.605034589767456\n",
      "Epoch: 31, Loss: 1.585066556930542\n",
      "Epoch: 32, Loss: 1.564220905303955\n",
      "Epoch: 33, Loss: 1.5437006950378418\n",
      "Epoch: 34, Loss: 1.5232982635498047\n",
      "Epoch: 35, Loss: 1.5029098987579346\n",
      "Epoch: 36, Loss: 1.4830694198608398\n",
      "Epoch: 37, Loss: 1.4627861976623535\n",
      "Epoch: 38, Loss: 1.4426360130310059\n",
      "Epoch: 39, Loss: 1.4225749969482422\n",
      "Epoch: 40, Loss: 1.4035667181015015\n",
      "Epoch: 41, Loss: 1.3843741416931152\n",
      "Epoch: 42, Loss: 1.3656495809555054\n",
      "Epoch: 43, Loss: 1.3472568988800049\n",
      "Epoch: 44, Loss: 1.3285503387451172\n",
      "Epoch: 45, Loss: 1.3095349073410034\n",
      "Epoch: 46, Loss: 1.2913340330123901\n",
      "Epoch: 47, Loss: 1.2729856967926025\n",
      "Epoch: 48, Loss: 1.2549519538879395\n",
      "Epoch: 49, Loss: 1.2379517555236816\n",
      "Epoch: 50, Loss: 1.2215889692306519\n",
      "Epoch: 51, Loss: 1.205173373222351\n",
      "Epoch: 52, Loss: 1.1890299320220947\n",
      "Epoch: 53, Loss: 1.173586368560791\n",
      "Epoch: 54, Loss: 1.15852689743042\n",
      "Epoch: 55, Loss: 1.1428117752075195\n",
      "Epoch: 56, Loss: 1.1271007061004639\n",
      "Epoch: 57, Loss: 1.1108967065811157\n",
      "Epoch: 58, Loss: 1.0949437618255615\n",
      "Epoch: 59, Loss: 1.079459309577942\n",
      "Epoch: 60, Loss: 1.064723014831543\n",
      "Epoch: 61, Loss: 1.0502322912216187\n",
      "Epoch: 62, Loss: 1.0353012084960938\n",
      "Epoch: 63, Loss: 1.0202867984771729\n",
      "Epoch: 64, Loss: 1.0053555965423584\n",
      "Epoch: 65, Loss: 0.9910423755645752\n",
      "Epoch: 66, Loss: 0.9774739146232605\n",
      "Epoch: 67, Loss: 0.963716447353363\n",
      "Epoch: 68, Loss: 0.9500551223754883\n",
      "Epoch: 69, Loss: 0.9367997050285339\n",
      "Epoch: 70, Loss: 0.9234554171562195\n",
      "Epoch: 71, Loss: 0.9100943803787231\n",
      "Epoch: 72, Loss: 0.897340714931488\n",
      "Epoch: 73, Loss: 0.8848196268081665\n",
      "Epoch: 74, Loss: 0.8722855448722839\n",
      "Epoch: 75, Loss: 0.8597891926765442\n",
      "Epoch: 76, Loss: 0.8480654954910278\n",
      "Epoch: 77, Loss: 0.836986780166626\n",
      "Epoch: 78, Loss: 0.8253492116928101\n",
      "Epoch: 79, Loss: 0.8132274150848389\n",
      "Epoch: 80, Loss: 0.8017137050628662\n",
      "Epoch: 81, Loss: 0.7908676266670227\n",
      "Epoch: 82, Loss: 0.7801454067230225\n",
      "Epoch: 83, Loss: 0.769124448299408\n",
      "Epoch: 84, Loss: 0.7587517499923706\n",
      "Epoch: 85, Loss: 0.7483648061752319\n",
      "Epoch: 86, Loss: 0.7377763986587524\n",
      "Epoch: 87, Loss: 0.7277857661247253\n",
      "Epoch: 88, Loss: 0.7186534404754639\n",
      "Epoch: 89, Loss: 0.7095484733581543\n",
      "Epoch: 90, Loss: 0.700594425201416\n",
      "Epoch: 91, Loss: 0.6920102834701538\n",
      "Epoch: 92, Loss: 0.6834051609039307\n",
      "Epoch: 93, Loss: 0.675033450126648\n",
      "Epoch: 94, Loss: 0.6662623286247253\n",
      "Epoch: 95, Loss: 0.657879114151001\n",
      "Epoch: 96, Loss: 0.649889349937439\n",
      "Epoch: 97, Loss: 0.642387866973877\n",
      "Epoch: 98, Loss: 0.6348876357078552\n",
      "Epoch: 99, Loss: 0.6273179054260254\n",
      "Epoch: 100, Loss: 0.6194693446159363\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training = True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fbe31",
   "metadata": {},
   "source": [
    "# Specialized Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b69b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class CustomDenseLayer(Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "# Usage in model\n",
    "model = Sequential([CustomDenseLayer(64), Dense(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4eca8a",
   "metadata": {},
   "source": [
    "# Advanced callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf2f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 510ms/step - accuracy: 0.0938 - loss: 10.2938\n",
      "End of epoch 0, loss: 8.548270225524902, accuracy: 0.20000000298023224\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1790 - loss: 8.9694   \n",
      "Epoch 2/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1562 - loss: 7.7693\n",
      "End of epoch 1, loss: 6.289118766784668, accuracy: 0.14000000059604645\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1445 - loss: 6.5664\n",
      "Epoch 3/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1562 - loss: 7.5532\n",
      "End of epoch 2, loss: 5.3624348640441895, accuracy: 0.1599999964237213\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1598 - loss: 5.8745\n",
      "Epoch 4/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0938 - loss: 4.2503\n",
      "End of epoch 3, loss: 3.4832923412323, accuracy: 0.14000000059604645\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1258 - loss: 3.7089\n",
      "Epoch 5/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0938 - loss: 2.3026\n",
      "End of epoch 4, loss: 2.398047685623169, accuracy: 0.15000000596046448\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1298 - loss: 2.3905 \n",
      "Epoch 6/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0938 - loss: 2.3026\n",
      "End of epoch 5, loss: 2.3025851249694824, accuracy: 0.14000000059604645\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1237 - loss: 2.3026\n",
      "Epoch 7/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0625 - loss: 2.3026\n",
      "End of epoch 6, loss: 2.3025851249694824, accuracy: 0.10999999940395355\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0961 - loss: 2.3026\n",
      "Epoch 8/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0625 - loss: 2.3026\n",
      "End of epoch 7, loss: 2.3025851249694824, accuracy: 0.10000000149011612\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0869 - loss: 2.3026\n",
      "Epoch 9/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0938 - loss: 2.3026\n",
      "End of epoch 8, loss: 2.3025851249694824, accuracy: 0.12999999523162842\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1176 - loss: 2.3026 \n",
      "Epoch 10/10\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0938 - loss: 2.3026\n",
      "End of epoch 9, loss: 2.3025851249694824, accuracy: 0.14000000059604645\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1237 - loss: 2.3026 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x228acb9abc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'\\nEnd of epoch {epoch}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n",
    "\n",
    "# Usage in model training\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=10, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d777c62",
   "metadata": {},
   "source": [
    "# Mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6b84df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1136 - loss: 9.1335   \n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1136 - loss: 9.3014  \n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1320 - loss: 9.0766  \n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1228 - loss: 8.9589  \n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1289 - loss: 9.0019 \n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1657 - loss: 8.8604 \n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1442 - loss: 8.7219  \n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1320 - loss: 9.1406  \n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1320 - loss: 9.3698  \n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1505 - loss: 9.0826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x228af3c1840>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential([Dense(64, activation='relu'), Dense(10)])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# TTraining the model\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imb-ai-engineering-KgAhWUw3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
